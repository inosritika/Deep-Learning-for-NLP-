{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"gpu","dataSources":[{"sourceId":8320655,"sourceType":"datasetVersion","datasetId":4942445}],"dockerImageVersionId":30698,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"!pip install -q bitsandbytes datasets accelerate loralib\n!pip install -q git+https://github.com/huggingface/transformers.git@main git+https://github.com/huggingface/peft.git","metadata":{"execution":{"iopub.status.busy":"2024-05-08T06:15:29.588455Z","iopub.execute_input":"2024-05-08T06:15:29.589344Z","iopub.status.idle":"2024-05-08T06:16:53.149739Z","shell.execute_reply.started":"2024-05-08T06:15:29.589300Z","shell.execute_reply":"2024-05-08T06:16:53.148414Z"},"trusted":true},"execution_count":1,"outputs":[]},{"cell_type":"code","source":"import torch\nfrom torch.utils.data import Dataset\nimport pandas as pd\nfrom transformers import AutoModelForCausalLM, AutoTokenizer, Trainer, TrainingArguments, DataCollatorForLanguageModeling\nfrom peft import LoraConfig, get_peft_model\n\n# Define the tokenizer and model globally\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\ntokenizer = AutoTokenizer.from_pretrained('microsoft/DialoGPT-medium')\ntokenizer.pad_token = tokenizer.eos_token\ntokenizer.padding_side = \"left\"\n\n# Define the model globally and specify the 'medium' model\nglobal_model = AutoModelForCausalLM.from_pretrained('microsoft/DialoGPT-medium')\nglobal_model.to(device)\n\nclass SpeechDataset(Dataset):\n    def __init__(self, df, tokenizer, device):\n        self.df = df\n        self.tokenizer = tokenizer\n        self.device = device\n\n    def __len__(self):\n        return len(self.df)\n\n    def __getitem__(self, index):\n        hate_speech = self.df.iloc[index][\"Hate Speech\"]\n        counterspeech = self.df.iloc[index][\"Counterspeech\"]\n        prompt = f\"{hate_speech} {self.tokenizer.eos_token} {counterspeech}\"\n        encoding = self.tokenizer(prompt, max_length=256, padding=\"max_length\", truncation=True, return_tensors=\"pt\")\n        return {\"input_ids\": encoding[\"input_ids\"].squeeze().to(self.device), \n                \"attention_mask\": encoding[\"attention_mask\"].squeeze().to(self.device), \n                \"labels\": encoding[\"input_ids\"].squeeze().to(self.device)}\n\nclass Generator:\n    def __init__(self, train_df, val_df, category, tokenizer, device):\n        self.model = global_model  # Use the globally defined model\n        config = LoraConfig(\n            r=16,  # Number of attention heads\n            lora_alpha=32,  # Alpha scaling\n            lora_dropout=0.05,\n            bias=\"none\",\n            task_type=\"CAUSAL_LM\"\n        )\n        self.model = get_peft_model(self.model, config)  # Apply PEFT configuration\n        self.train_dataset = SpeechDataset(train_df, tokenizer, device)\n        self.val_dataset = SpeechDataset(val_df, tokenizer, device)\n        self.category = category\n        self.tokenizer = tokenizer\n        self.device = device\n\n    def train(self, num_epochs=3):\n        training_args = TrainingArguments(\n            output_dir=f\"./results_{self.category}\",\n            evaluation_strategy=\"epoch\",\n            warmup_steps=100,\n            learning_rate=1e-3,\n            per_device_train_batch_size=4,\n            per_device_eval_batch_size=4,\n            weight_decay=0.01,\n            num_train_epochs=num_epochs,\n            logging_dir='./logs',\n            fp16=True,\n            gradient_accumulation_steps=4,\n            max_steps=200,\n            logging_steps=1\n        )\n        trainer = Trainer(\n            model=self.model,\n            args=training_args,\n            train_dataset=self.train_dataset,\n            eval_dataset=self.val_dataset,\n            data_collator=DataCollatorForLanguageModeling(self.tokenizer, mlm=False)\n        )\n        trainer.train()\n\ndef generate_counterspeech(generators, hate_speech, category):\n    generator = generators[category]\n    prompt = f\"{hate_speech}{generator.tokenizer.eos_token}\"\n    input_ids = generator.tokenizer.encode(prompt, return_tensors='pt').to(generator.device)\n    output_ids = generator.model.generate(input_ids, max_length=512, pad_token_id=generator.tokenizer.eos_token_id)\n    output = generator.tokenizer.decode(output_ids[:, input_ids.shape[-1]:][0], skip_special_tokens=True)\n    return output\n\n# Load your data\ntrain_df = pd.read_csv(\"/kaggle/input/counterspeech/train.csv\")\nval_df = pd.read_csv(\"/kaggle/input/counterspeech/val.csv\")\n\ncategories = ['Positive', 'Denouncing', 'Facts', 'Question', 'Humor']\ngenerators = {}\n\nfor category in categories:\n    train_subdf = train_df[train_df['Category'] == category].reset_index(drop=True)\n    val_subdf = val_df[val_df['Category'] == category].reset_index(drop=True)\n    generator = Generator(train_subdf, val_subdf, category, tokenizer, device)\n    generators[category] = generator  \n\nfor category in categories:\n    generators[category].train()\n\ndef inference(df_name, generators):\n    df = pd.read_csv(df_name)\n#     df = df.head(10)\n    generate = lambda row: generate_counterspeech(generators, row[\"Hate Speech\"], row[\"Category\"])\n    df[\"Counterspeech\"] = df.apply(generate, axis=1)\n    return df\n\ntest_df = inference(\"/kaggle/input/counterspeech/test.csv\", generators)\nprint(test_df)\ntest_df.to_csv('/kaggle/working/output.csv', index=False)\n","metadata":{"execution":{"iopub.status.busy":"2024-05-08T06:16:53.199072Z","iopub.execute_input":"2024-05-08T06:16:53.199400Z"},"trusted":true},"execution_count":null,"outputs":[{"name":"stderr","text":"2024-05-08 06:16:59.919374: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:9261] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n2024-05-08 06:16:59.919469: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:607] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n2024-05-08 06:17:00.041103: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1515] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"tokenizer_config.json:   0%|          | 0.00/614 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"62278ee64af8421ab99d06643992e591"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"vocab.json:   0%|          | 0.00/1.04M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"5eeba7cbf18d4f6aa33fce0a7622f471"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"merges.txt:   0%|          | 0.00/456k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"7814366a4c3e49f08794b929beae0ba3"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"config.json:   0%|          | 0.00/642 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"5cc7ac00af5c4b7aac56c116dd6479cb"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"pytorch_model.bin:   0%|          | 0.00/863M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"64c104c7948a4ae4aec0871b665f2f83"}},"metadata":{}},{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/torch/_utils.py:831: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()\n  return self.fget.__get__(instance, owner)()\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"generation_config.json:   0%|          | 0.00/124 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"a6e6d74ff90a46028567277de8f1b40e"}},"metadata":{}},{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/peft/tuners/lora/layer.py:1090: UserWarning: fan_in_fan_out is set to False but the target module is `Conv1D`. Setting fan_in_fan_out to True.\n  warnings.warn(\n/opt/conda/lib/python3.10/site-packages/transformers/training_args.py:1474: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of 🤗 Transformers. Use `eval_strategy` instead\n  warnings.warn(\nmax_steps is given, it will override any value given in num_train_epochs\n\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m The `run_name` is currently set to the same value as `TrainingArguments.output_dir`. If this was not intended, please specify a different run name by setting the `TrainingArguments.run_name` parameter.\n\u001b[34m\u001b[1mwandb\u001b[0m: Logging into wandb.ai. (Learn how to deploy a W&B server locally: https://wandb.me/wandb-server)\n\u001b[34m\u001b[1mwandb\u001b[0m: You can find your API key in your browser here: https://wandb.ai/authorize\n\u001b[34m\u001b[1mwandb\u001b[0m: Paste an API key from your profile and hit enter, or press ctrl+c to quit:","output_type":"stream"},{"output_type":"stream","name":"stdin","text":"  ········································\n"},{"name":"stderr","text":"\u001b[34m\u001b[1mwandb\u001b[0m: Appending key for api.wandb.ai to your netrc file: /root/.netrc\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"wandb version 0.17.0 is available!  To upgrade, please run:\n $ pip install wandb --upgrade"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Tracking run with wandb version 0.16.6"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Run data is saved locally in <code>/kaggle/working/wandb/run-20240508_061742-eizrkip0</code>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Syncing run <strong><a href='https://wandb.ai/iitdelhi2020/huggingface/runs/eizrkip0' target=\"_blank\">./results_Positive</a></strong> to <a href='https://wandb.ai/iitdelhi2020/huggingface' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":" View project at <a href='https://wandb.ai/iitdelhi2020/huggingface' target=\"_blank\">https://wandb.ai/iitdelhi2020/huggingface</a>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":" View run at <a href='https://wandb.ai/iitdelhi2020/huggingface/runs/eizrkip0' target=\"_blank\">https://wandb.ai/iitdelhi2020/huggingface/runs/eizrkip0</a>"},"metadata":{}},{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n  warnings.warn(\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"\n    <div>\n      \n      <progress value='200' max='200' style='width:300px; height:20px; vertical-align: middle;'></progress>\n      [200/200 05:28, Epoch 4/5]\n    </div>\n    <table border=\"1\" class=\"dataframe\">\n  <thead>\n <tr style=\"text-align: left;\">\n      <th>Epoch</th>\n      <th>Training Loss</th>\n      <th>Validation Loss</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <td>0</td>\n      <td>5.643400</td>\n      <td>5.086633</td>\n    </tr>\n    <tr>\n      <td>2</td>\n      <td>3.227900</td>\n      <td>3.260252</td>\n    </tr>\n    <tr>\n      <td>4</td>\n      <td>3.195800</td>\n      <td>3.080157</td>\n    </tr>\n  </tbody>\n</table><p>"},"metadata":{}},{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/transformers/training_args.py:1474: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of 🤗 Transformers. Use `eval_strategy` instead\n  warnings.warn(\nmax_steps is given, it will override any value given in num_train_epochs\n/opt/conda/lib/python3.10/site-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n  warnings.warn(\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"\n    <div>\n      \n      <progress value='200' max='200' style='width:300px; height:20px; vertical-align: middle;'></progress>\n      [200/200 05:27, Epoch 4/5]\n    </div>\n    <table border=\"1\" class=\"dataframe\">\n  <thead>\n <tr style=\"text-align: left;\">\n      <th>Epoch</th>\n      <th>Training Loss</th>\n      <th>Validation Loss</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <td>1</td>\n      <td>3.352300</td>\n      <td>3.271511</td>\n    </tr>\n    <tr>\n      <td>2</td>\n      <td>3.437600</td>\n      <td>3.220133</td>\n    </tr>\n    <tr>\n      <td>3</td>\n      <td>3.225100</td>\n      <td>3.151061</td>\n    </tr>\n    <tr>\n      <td>4</td>\n      <td>3.248700</td>\n      <td>3.103521</td>\n    </tr>\n  </tbody>\n</table><p>"},"metadata":{}},{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/transformers/training_args.py:1474: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of 🤗 Transformers. Use `eval_strategy` instead\n  warnings.warn(\nmax_steps is given, it will override any value given in num_train_epochs\n/opt/conda/lib/python3.10/site-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n  warnings.warn(\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"\n    <div>\n      \n      <progress value='200' max='200' style='width:300px; height:20px; vertical-align: middle;'></progress>\n      [200/200 05:26, Epoch 1/2]\n    </div>\n    <table border=\"1\" class=\"dataframe\">\n  <thead>\n <tr style=\"text-align: left;\">\n      <th>Epoch</th>\n      <th>Training Loss</th>\n      <th>Validation Loss</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <td>0</td>\n      <td>2.930000</td>\n      <td>2.839057</td>\n    </tr>\n    <tr>\n      <td>1</td>\n      <td>3.154800</td>\n      <td>2.783650</td>\n    </tr>\n  </tbody>\n</table><p>"},"metadata":{}},{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/transformers/training_args.py:1474: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of 🤗 Transformers. Use `eval_strategy` instead\n  warnings.warn(\nmax_steps is given, it will override any value given in num_train_epochs\n/opt/conda/lib/python3.10/site-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n  warnings.warn(\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"\n    <div>\n      \n      <progress value='200' max='200' style='width:300px; height:20px; vertical-align: middle;'></progress>\n      [200/200 05:22, Epoch 2/3]\n    </div>\n    <table border=\"1\" class=\"dataframe\">\n  <thead>\n <tr style=\"text-align: left;\">\n      <th>Epoch</th>\n      <th>Training Loss</th>\n      <th>Validation Loss</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <td>0</td>\n      <td>3.099600</td>\n      <td>2.734032</td>\n    </tr>\n    <tr>\n      <td>2</td>\n      <td>2.522900</td>\n      <td>2.677843</td>\n    </tr>\n  </tbody>\n</table><p>"},"metadata":{}},{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/transformers/training_args.py:1474: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of 🤗 Transformers. Use `eval_strategy` instead\n  warnings.warn(\nmax_steps is given, it will override any value given in num_train_epochs\n/opt/conda/lib/python3.10/site-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n  warnings.warn(\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"\n    <div>\n      \n      <progress value='200' max='200' style='width:300px; height:20px; vertical-align: middle;'></progress>\n      [200/200 05:23, Epoch 6/7]\n    </div>\n    <table border=\"1\" class=\"dataframe\">\n  <thead>\n <tr style=\"text-align: left;\">\n      <th>Epoch</th>\n      <th>Training Loss</th>\n      <th>Validation Loss</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <td>1</td>\n      <td>3.434700</td>\n      <td>3.338650</td>\n    </tr>\n    <tr>\n      <td>2</td>\n      <td>3.333500</td>\n      <td>3.286829</td>\n    </tr>\n    <tr>\n      <td>3</td>\n      <td>3.412600</td>\n      <td>3.240785</td>\n    </tr>\n    <tr>\n      <td>4</td>\n      <td>3.142200</td>\n      <td>3.237777</td>\n    </tr>\n    <tr>\n      <td>5</td>\n      <td>2.951800</td>\n      <td>3.224050</td>\n    </tr>\n    <tr>\n      <td>6</td>\n      <td>2.569700</td>\n      <td>3.231275</td>\n    </tr>\n  </tbody>\n</table><p>"},"metadata":{}},{"name":"stderr","text":"A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n","output_type":"stream"}]}]}