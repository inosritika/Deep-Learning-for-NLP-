{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "2094f1f7",
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
    "execution": {
     "iopub.execute_input": "2024-01-24T06:27:22.949887Z",
     "iopub.status.busy": "2024-01-24T06:27:22.949334Z",
     "iopub.status.idle": "2024-01-24T06:27:23.885307Z",
     "shell.execute_reply": "2024-01-24T06:27:23.883725Z"
    },
    "papermill": {
     "duration": 0.946512,
     "end_time": "2024-01-24T06:27:23.888430",
     "exception": false,
     "start_time": "2024-01-24T06:27:22.941918",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# This Python 3 environment comes with many helpful analytics libraries installed\n",
    "# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n",
    "# For example, here's several helpful packages to load\n",
    "\n",
    "import numpy as np \n",
    "from tqdm import tqdm\n",
    "import pandas as pd\n",
    "import random\n",
    "import ast\n",
    "\n",
    "\n",
    "# Input data files are available in the read-only \"../input/\" directory\n",
    "# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n",
    "\n",
    "# import os\n",
    "# for dirname, _, filenames in os.walk('/kaggle/input'):\n",
    "#     for filename in filenames:\n",
    "#         print(os.path.join(dirname, filename))\n",
    "\n",
    "# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n",
    "# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "0c39a2e5",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-01-24T06:27:23.901548Z",
     "iopub.status.busy": "2024-01-24T06:27:23.900911Z",
     "iopub.status.idle": "2024-01-24T06:27:37.084749Z",
     "shell.execute_reply": "2024-01-24T06:27:37.081971Z"
    },
    "papermill": {
     "duration": 13.193734,
     "end_time": "2024-01-24T06:27:37.087767",
     "exception": false,
     "start_time": "2024-01-24T06:27:23.894033",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "47340it [00:13, 3525.58it/s]                                                                                           \n"
     ]
    }
   ],
   "source": [
    "df = pd.read_csv('C:\\\\Users\\\\Dell\\\\Desktop\\\\nlp_assignment1\\\\train.csv') # loading training data\n",
    "data = []\n",
    "for index, row in tqdm(df.iterrows(), total=1000):\n",
    "    data.append(ast.literal_eval(row['tagged_sentence'])) # changing data-type of entries from 'str' to 'list'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a7d94230",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-01-24T06:27:37.124238Z",
     "iopub.status.busy": "2024-01-24T06:27:37.123832Z",
     "iopub.status.idle": "2024-01-24T06:27:37.748538Z",
     "shell.execute_reply": "2024-01-24T06:27:37.746966Z"
    },
    "papermill": {
     "duration": 0.645769,
     "end_time": "2024-01-24T06:27:37.751054",
     "exception": false,
     "start_time": "2024-01-24T06:27:37.105285",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "4000it [00:00, 8068.56it/s]                                                                                            \n"
     ]
    }
   ],
   "source": [
    "df = pd.read_csv('C:\\\\Users\\\\Dell\\\\Desktop\\\\nlp_assignment1\\\\test_small.csv') # loading test data\n",
    "test_data = {} \n",
    "for index, row in tqdm(df.iterrows(), total=1000):\n",
    "    test_data[row['id']] = ast.literal_eval(row['untagged_sentence']) # changing data-type of entries from 'str' to 'list'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "8b55320d",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-01-24T06:27:37.788352Z",
     "iopub.status.busy": "2024-01-24T06:27:37.787926Z",
     "iopub.status.idle": "2024-01-24T06:27:37.795498Z",
     "shell.execute_reply": "2024-01-24T06:27:37.794586Z"
    },
    "papermill": {
     "duration": 0.029871,
     "end_time": "2024-01-24T06:27:37.798593",
     "exception": false,
     "start_time": "2024-01-24T06:27:37.768722",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TOKEN -> TAG\n",
      "...\n",
      "So > CS\n",
      "that > DT\n",
      "had > HV\n",
      "been > BE\n",
      "his > PP\n",
      "difficulty > NN\n",
      ". > .\n"
     ]
    }
   ],
   "source": [
    "def display_data(sentence_index):\n",
    "    '''\n",
    "        Input : 'sentence_index' (int) -> index of a sentence in training data\n",
    "        Output: None\n",
    "    '''\n",
    "    sentence = data[sentence_index]\n",
    "    print(\"TOKEN -> TAG\")\n",
    "    print('...')\n",
    "    for token, tag in sentence:\n",
    "        print(token, '>', tag)\n",
    "sentence_index = random.choice(range(len(data)))\n",
    "display_data(sentence_index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "67258b70",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-01-24T06:27:37.835773Z",
     "iopub.status.busy": "2024-01-24T06:27:37.835291Z",
     "iopub.status.idle": "2024-01-24T06:27:38.636826Z",
     "shell.execute_reply": "2024-01-24T06:27:38.635357Z"
    },
    "papermill": {
     "duration": 0.824303,
     "end_time": "2024-01-24T06:27:38.640460",
     "exception": false,
     "start_time": "2024-01-24T06:27:37.816157",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# cell to show the frequency of each distinct (slack or native) present in the training data\n",
    "from collections import Counter\n",
    "distinct_tags = []\n",
    "word_tags = []\n",
    "def store_tags():\n",
    "    \n",
    "    global distinct_tags\n",
    "    global word_tags\n",
    "    \n",
    "    for sent in data:\n",
    "        word_tags.append(('START','START'))\n",
    "        for words, tag in sent:\n",
    "            word_tags.extend([(tag, words)])\n",
    "        word_tags.append(('END','END'))\n",
    "    \n",
    "store_tags()\n",
    "tags=[]\n",
    "for tag, words in word_tags:\n",
    "    tags.append(tag)\n",
    "distinct_tags=list(set(tags))\n",
    "count_tags = {}\n",
    "for tag, count in Counter(tags).items():\n",
    "    count_tags[tag] = count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "e34bc71d",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-01-24T06:27:38.679471Z",
     "iopub.status.busy": "2024-01-24T06:27:38.678964Z",
     "iopub.status.idle": "2024-01-24T06:27:39.303310Z",
     "shell.execute_reply": "2024-01-24T06:27:39.302099Z"
    },
    "papermill": {
     "duration": 0.647821,
     "end_time": "2024-01-24T06:27:39.306365",
     "exception": false,
     "start_time": "2024-01-24T06:27:38.658544",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 864x432 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "plt.figure(figsize=(12, 6))\n",
    "plt.xticks(rotation='vertical')\n",
    "plt.bar(range(len(count_tags)), list(count_tags.values()), align='center')\n",
    "plt.xticks(range(len(count_tags)), list(count_tags.keys()))\n",
    "plt.xlabel('Tag')\n",
    "plt.ylabel('Count')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "8e12c902",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-01-24T06:27:39.346432Z",
     "iopub.status.busy": "2024-01-24T06:27:39.345969Z",
     "iopub.status.idle": "2024-01-24T06:27:39.352465Z",
     "shell.execute_reply": "2024-01-24T06:27:39.351514Z"
    },
    "papermill": {
     "duration": 0.029691,
     "end_time": "2024-01-24T06:27:39.355084",
     "exception": false,
     "start_time": "2024-01-24T06:27:39.325393",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "submission = {'id': [], 'tagged_sentence' : []} # dictionary to store tag predictions\n",
    "# NOTE ---> ensure that tagged_sentence's corresponing 'id' is same as 'id' of corresponding 'untagged_sentence' in training data\n",
    "def store_submission(sent_id, tagged_sentence):\n",
    "    \n",
    "    global submission\n",
    "    submission['id'].append(sent_id)\n",
    "    submission['tagged_sentence'].append(tagged_sentence)\n",
    "    \n",
    "def clear_submission():\n",
    "    global submission\n",
    "    submission = {'id': [], 'tagged_sentence' : []}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "f8eff508",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-01-24T06:27:39.395078Z",
     "iopub.status.busy": "2024-01-24T06:27:39.394215Z",
     "iopub.status.idle": "2024-01-24T06:27:39.399753Z",
     "shell.execute_reply": "2024-01-24T06:27:39.398417Z"
    },
    "papermill": {
     "duration": 0.028361,
     "end_time": "2024-01-24T06:27:39.402206",
     "exception": false,
     "start_time": "2024-01-24T06:27:39.373845",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def hmm_tagger_util(sent_id, untagged_sentence):\n",
    "    store_submission(sent_id, tagged_sentence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "38ef359d",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-01-24T06:27:39.443765Z",
     "iopub.status.busy": "2024-01-24T06:27:39.442853Z",
     "iopub.status.idle": "2024-01-24T06:27:39.448258Z",
     "shell.execute_reply": "2024-01-24T06:27:39.447331Z"
    },
    "papermill": {
     "duration": 0.02932,
     "end_time": "2024-01-24T06:27:39.450880",
     "exception": false,
     "start_time": "2024-01-24T06:27:39.421560",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def memm_tagger_util(sent_id, untagged_sentence):\n",
    "    store_submission(sent_id, tagged_sentence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "851cac9f",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-01-24T06:27:39.490144Z",
     "iopub.status.busy": "2024-01-24T06:27:39.489123Z",
     "iopub.status.idle": "2024-01-24T06:27:39.495510Z",
     "shell.execute_reply": "2024-01-24T06:27:39.494649Z"
    },
    "papermill": {
     "duration": 0.028832,
     "end_time": "2024-01-24T06:27:39.498054",
     "exception": false,
     "start_time": "2024-01-24T06:27:39.469222",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# cell to implement tagger that allots random tags to words in a sentence\n",
    "\n",
    "def random_tagger_util(sent_id, untagged_sentence):\n",
    "    if(sent_id in list(submission['id'])):\n",
    "        return\n",
    "    tagged_sentence = []\n",
    "    for word in untagged_sentence:\n",
    "        tagged_sentence.append((word, random.choice(distinct_tags)))\n",
    "    store_submission(sent_id, tagged_sentence)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "c6e61b71",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-01-24T06:27:39.536647Z",
     "iopub.status.busy": "2024-01-24T06:27:39.535871Z",
     "iopub.status.idle": "2024-01-24T06:27:39.810083Z",
     "shell.execute_reply": "2024-01-24T06:27:39.808451Z"
    },
    "papermill": {
     "duration": 0.297769,
     "end_time": "2024-01-24T06:27:39.814010",
     "exception": false,
     "start_time": "2024-01-24T06:27:39.516241",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████████████████████████████████████| 4000/4000 [00:00<00:00, 16863.78it/s]\n"
     ]
    }
   ],
   "source": [
    "for sent_id in tqdm(list(test_data.keys())):\n",
    "    sent = test_data[sent_id]\n",
    "    random_tagger_util(sent_id, sent)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "6f0d2547",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-01-24T06:27:39.855912Z",
     "iopub.status.busy": "2024-01-24T06:27:39.855380Z",
     "iopub.status.idle": "2024-01-24T06:27:39.995303Z",
     "shell.execute_reply": "2024-01-24T06:27:39.994062Z"
    },
    "papermill": {
     "duration": 0.16399,
     "end_time": "2024-01-24T06:27:39.998445",
     "exception": false,
     "start_time": "2024-01-24T06:27:39.834455",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# path_to_directory = '/kaggle/working/'\n",
    "# pd.DataFrame(submission).to_csv(path_to_directory +' sample_submission.csv', index = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "b10f4bdb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[' ',\n",
       " '!',\n",
       " '$.03',\n",
       " '$1',\n",
       " '$1,000',\n",
       " '$1,500',\n",
       " '$10',\n",
       " '$10,000',\n",
       " '$100',\n",
       " '$135',\n",
       " '$14',\n",
       " '$15',\n",
       " '$15,000',\n",
       " '$150',\n",
       " '$2',\n",
       " '$2,000',\n",
       " '$20',\n",
       " '$20,000',\n",
       " '$200',\n",
       " '$25',\n",
       " '$25,000',\n",
       " '$250',\n",
       " '$28',\n",
       " '$3,000',\n",
       " '$30,000',\n",
       " '$300',\n",
       " '$4',\n",
       " '$40',\n",
       " '$400',\n",
       " '$45',\n",
       " '$450',\n",
       " '$5',\n",
       " '$5,000',\n",
       " '$5,000,000',\n",
       " '$50',\n",
       " '$500',\n",
       " '$500,000',\n",
       " '$5000',\n",
       " '$600',\n",
       " '$7',\n",
       " '$700',\n",
       " '$75',\n",
       " '$750',\n",
       " '$800',\n",
       " '$900',\n",
       " '&',\n",
       " \"'\",\n",
       " \"''\",\n",
       " \"'48\",\n",
       " \"'50\",\n",
       " \"'52\",\n",
       " \"'61\",\n",
       " \"'em\",\n",
       " \"'im\",\n",
       " \"'round\",\n",
       " '(',\n",
       " ')',\n",
       " '**ya',\n",
       " '**yb',\n",
       " '**yc',\n",
       " '**yf',\n",
       " '**yl',\n",
       " '**yt',\n",
       " '**zg',\n",
       " ',',\n",
       " '-',\n",
       " '--',\n",
       " '--n--',\n",
       " '--unk--',\n",
       " '--unk_adj--',\n",
       " '--unk_adv--',\n",
       " '--unk_digit--',\n",
       " '--unk_noun--',\n",
       " '--unk_punct--',\n",
       " '--unk_upper--',\n",
       " '--unk_verb--',\n",
       " '.',\n",
       " '/',\n",
       " '0',\n",
       " '0.1',\n",
       " '0.3',\n",
       " '0.4',\n",
       " '0.5',\n",
       " '0.8',\n",
       " '1',\n",
       " \"1''\",\n",
       " '1,000',\n",
       " '1,500',\n",
       " '1-1/2',\n",
       " '1.1',\n",
       " '1.5',\n",
       " '1.8',\n",
       " '1/2',\n",
       " \"1/2''\",\n",
       " '1/4',\n",
       " \"1/4''\",\n",
       " \"1/8''\",\n",
       " '10',\n",
       " '10%',\n",
       " \"10''\",\n",
       " '10,000',\n",
       " '10-year',\n",
       " '10-year-old',\n",
       " '100',\n",
       " '100%',\n",
       " '100,000',\n",
       " '1000',\n",
       " '104',\n",
       " '1040',\n",
       " '1040A',\n",
       " '109-degrees-F',\n",
       " '11',\n",
       " '110',\n",
       " '1105',\n",
       " '111',\n",
       " '114',\n",
       " '11:20',\n",
       " '11th',\n",
       " '12',\n",
       " '12%',\n",
       " \"12''\",\n",
       " '12-gauge',\n",
       " '120',\n",
       " '121',\n",
       " '128',\n",
       " '13',\n",
       " '130',\n",
       " '133',\n",
       " '13th',\n",
       " '14',\n",
       " '14%',\n",
       " '14-1/2',\n",
       " '140',\n",
       " '14th',\n",
       " '15',\n",
       " '15%',\n",
       " '150',\n",
       " '154',\n",
       " '1582',\n",
       " '15th',\n",
       " '16',\n",
       " '16,000',\n",
       " '160',\n",
       " '1600',\n",
       " '1602',\n",
       " '1609',\n",
       " '1611',\n",
       " '1625',\n",
       " '1629',\n",
       " '1632',\n",
       " '1643',\n",
       " '168',\n",
       " '16th',\n",
       " '17',\n",
       " '1700',\n",
       " '1776',\n",
       " '1783',\n",
       " '1788',\n",
       " '1792',\n",
       " '1793',\n",
       " '17th',\n",
       " '18',\n",
       " '180',\n",
       " '1801',\n",
       " '1803',\n",
       " '1810',\n",
       " '1812',\n",
       " '1815',\n",
       " '1817',\n",
       " '1818',\n",
       " '1819',\n",
       " '1820',\n",
       " '1821',\n",
       " '1832',\n",
       " '1834',\n",
       " '1840',\n",
       " '1844',\n",
       " '1845',\n",
       " '1848',\n",
       " '1850',\n",
       " '1859',\n",
       " '1861',\n",
       " '1863',\n",
       " '1864',\n",
       " '1865',\n",
       " '1868',\n",
       " '1875',\n",
       " '1876',\n",
       " '1882',\n",
       " '1883',\n",
       " '1885',\n",
       " '1887',\n",
       " '1888',\n",
       " '1890',\n",
       " '1891',\n",
       " '1892',\n",
       " '1893',\n",
       " '1895',\n",
       " '1896',\n",
       " '1897',\n",
       " '18th',\n",
       " '19',\n",
       " '1900',\n",
       " '1904',\n",
       " '1905',\n",
       " '1908',\n",
       " '1909',\n",
       " '1910',\n",
       " '1911',\n",
       " '1912',\n",
       " '1913',\n",
       " '1914',\n",
       " '1915',\n",
       " '1916',\n",
       " '1917',\n",
       " '1919',\n",
       " '1920',\n",
       " \"1920's\",\n",
       " '1921',\n",
       " '1922',\n",
       " '1923',\n",
       " '1924',\n",
       " '1925',\n",
       " '1926',\n",
       " '1927',\n",
       " '1928',\n",
       " '1929',\n",
       " '1930',\n",
       " \"1930's\",\n",
       " '1931',\n",
       " '1933',\n",
       " '1934',\n",
       " '1935',\n",
       " '1936',\n",
       " '1937',\n",
       " '1938',\n",
       " '1939',\n",
       " '1940',\n",
       " '1941',\n",
       " '1942',\n",
       " '1943',\n",
       " '1944',\n",
       " '1945',\n",
       " '1946',\n",
       " '1947',\n",
       " '1948',\n",
       " '1949',\n",
       " '1950',\n",
       " \"1950's\",\n",
       " '1950s',\n",
       " '1951',\n",
       " '1952',\n",
       " '1953',\n",
       " '1954',\n",
       " '1955',\n",
       " '1956',\n",
       " '1957',\n",
       " '1958',\n",
       " '1959',\n",
       " '1959-1960',\n",
       " '1960',\n",
       " \"1960's\",\n",
       " '1961',\n",
       " '1962',\n",
       " '1963',\n",
       " '1965',\n",
       " '1966',\n",
       " '1980',\n",
       " '19th',\n",
       " '2',\n",
       " '2%',\n",
       " '2,000',\n",
       " '2-1/2',\n",
       " '2-56',\n",
       " '2-year-old',\n",
       " '2.1',\n",
       " '2.6',\n",
       " '20',\n",
       " '20%',\n",
       " '20,000',\n",
       " '200',\n",
       " '200,000',\n",
       " '2000',\n",
       " '203',\n",
       " '20th',\n",
       " '21',\n",
       " '21st',\n",
       " '22',\n",
       " '23',\n",
       " '230',\n",
       " '23d',\n",
       " '23rd',\n",
       " '24',\n",
       " '24-hr.',\n",
       " '24th',\n",
       " '25',\n",
       " '25%',\n",
       " '25,000',\n",
       " '250',\n",
       " '256',\n",
       " '26',\n",
       " '27',\n",
       " '275',\n",
       " '28',\n",
       " '280',\n",
       " '28th',\n",
       " '29',\n",
       " '29th',\n",
       " '2:01.1',\n",
       " '2:30',\n",
       " '2:30.3-:36',\n",
       " '2:33',\n",
       " '2:35',\n",
       " '2:36',\n",
       " '2:37',\n",
       " '2d',\n",
       " '2nd',\n",
       " '3',\n",
       " '3%',\n",
       " '3,000',\n",
       " '3,500',\n",
       " '3.15',\n",
       " '3.5',\n",
       " '30',\n",
       " '30%',\n",
       " '30,000',\n",
       " '300',\n",
       " '300,000',\n",
       " '30th',\n",
       " '31',\n",
       " '32',\n",
       " '33',\n",
       " '34',\n",
       " '348',\n",
       " '35',\n",
       " '350',\n",
       " '353',\n",
       " '36',\n",
       " '360',\n",
       " '368(a)(1)',\n",
       " '37',\n",
       " '375-degrees-C',\n",
       " '38',\n",
       " '381',\n",
       " '381(a)',\n",
       " '39',\n",
       " '3rd',\n",
       " '4',\n",
       " '4-H',\n",
       " '4-inch',\n",
       " '4.2',\n",
       " '4.3',\n",
       " '40',\n",
       " '40%',\n",
       " '40,000',\n",
       " '400',\n",
       " '41',\n",
       " '42',\n",
       " '43',\n",
       " '45',\n",
       " '45-degree',\n",
       " '450',\n",
       " '46',\n",
       " '47',\n",
       " '48',\n",
       " '49',\n",
       " '4:30',\n",
       " '4th',\n",
       " '5',\n",
       " '5%',\n",
       " '5,000',\n",
       " '5.7',\n",
       " '50',\n",
       " '50%',\n",
       " '50-megaton',\n",
       " '500',\n",
       " '500,000',\n",
       " '5000',\n",
       " '51',\n",
       " '52',\n",
       " '54',\n",
       " '55',\n",
       " '56',\n",
       " '57',\n",
       " '6',\n",
       " '6,000',\n",
       " '6-3',\n",
       " '6-hr.',\n",
       " '60',\n",
       " '60,000',\n",
       " '600',\n",
       " '61',\n",
       " '63',\n",
       " '64',\n",
       " '65',\n",
       " '68',\n",
       " '69',\n",
       " '6th',\n",
       " '7',\n",
       " '7-1',\n",
       " '7.5',\n",
       " '70',\n",
       " '70%',\n",
       " '70,000',\n",
       " '700',\n",
       " '707',\n",
       " '7070',\n",
       " '7070/7074',\n",
       " '71',\n",
       " '72',\n",
       " '73',\n",
       " '74',\n",
       " '75',\n",
       " '75%',\n",
       " '75,000',\n",
       " '750',\n",
       " '76',\n",
       " '77',\n",
       " '7:30',\n",
       " '7th',\n",
       " '8',\n",
       " '8,000',\n",
       " '80',\n",
       " '800',\n",
       " '800,000',\n",
       " '81',\n",
       " '83',\n",
       " '85',\n",
       " '86',\n",
       " '8:30',\n",
       " '8th',\n",
       " '9',\n",
       " '90',\n",
       " '90%',\n",
       " '90-degrees',\n",
       " '92',\n",
       " '93',\n",
       " '95',\n",
       " '96',\n",
       " '97',\n",
       " '98',\n",
       " '99',\n",
       " '9th',\n",
       " ':',\n",
       " ';',\n",
       " '?',\n",
       " 'A',\n",
       " \"A's\",\n",
       " 'A-Z',\n",
       " 'A.',\n",
       " 'A.B.',\n",
       " 'A.D.',\n",
       " 'A.L.A.M.',\n",
       " 'A.M.',\n",
       " 'A.M.A.',\n",
       " 'ADC',\n",
       " 'AFL-CIO',\n",
       " 'AIA',\n",
       " 'AID',\n",
       " 'AIMO',\n",
       " 'AM',\n",
       " 'AP',\n",
       " 'AWOC',\n",
       " 'Aaron',\n",
       " 'Abbey',\n",
       " 'Abe',\n",
       " 'Abel',\n",
       " 'About',\n",
       " 'Above',\n",
       " 'Abraham',\n",
       " 'Abstract',\n",
       " 'Abstraction',\n",
       " 'Academy',\n",
       " 'Acala',\n",
       " 'Accacia',\n",
       " 'According',\n",
       " 'Accordingly',\n",
       " 'Achievement',\n",
       " 'Acropolis',\n",
       " 'Across',\n",
       " 'Act',\n",
       " 'Acting',\n",
       " 'Action',\n",
       " 'Active',\n",
       " 'Activities',\n",
       " 'Actual',\n",
       " 'Actually',\n",
       " 'Ada',\n",
       " \"Ada's\",\n",
       " 'Adam',\n",
       " 'Adams',\n",
       " 'Add',\n",
       " 'Additional',\n",
       " 'Adelia',\n",
       " 'Adenauer',\n",
       " 'Adios',\n",
       " 'Adjusted',\n",
       " 'Adlai',\n",
       " 'Administration',\n",
       " 'Administrative',\n",
       " 'Ado',\n",
       " 'Adolf',\n",
       " 'Adoniram',\n",
       " 'Adrian',\n",
       " 'Adriatic',\n",
       " 'Advisory',\n",
       " 'Advocate',\n",
       " 'Aegean',\n",
       " 'Aerospace',\n",
       " 'Af',\n",
       " 'Af-fold',\n",
       " 'Af-stage',\n",
       " 'Affairs',\n",
       " 'Africa',\n",
       " 'African',\n",
       " 'Africans',\n",
       " 'Afro-Asian',\n",
       " 'After',\n",
       " 'Afterwards',\n",
       " 'Again',\n",
       " 'Against',\n",
       " 'Age',\n",
       " 'Agency',\n",
       " 'Ages',\n",
       " 'Agnese',\n",
       " 'Agreeable',\n",
       " 'Agreement',\n",
       " 'Agriculture',\n",
       " 'Ah',\n",
       " 'Ahmet',\n",
       " 'Aid',\n",
       " 'Aids',\n",
       " \"Ain't\",\n",
       " 'Air',\n",
       " 'Aircraft',\n",
       " 'Airport',\n",
       " 'Aj',\n",
       " 'Al',\n",
       " 'Ala.',\n",
       " 'Alabama',\n",
       " 'Alan',\n",
       " 'Alaska',\n",
       " 'Alastor',\n",
       " 'Albania',\n",
       " 'Albany',\n",
       " 'Albert',\n",
       " 'Albright',\n",
       " 'Alec',\n",
       " \"Alec's\",\n",
       " 'Alex',\n",
       " \"Alex's\",\n",
       " 'Alexander',\n",
       " \"Alexander's\",\n",
       " 'Alexandria',\n",
       " 'Alfred',\n",
       " \"Alfred's\",\n",
       " 'Alfredo',\n",
       " 'Algerian',\n",
       " 'Algol',\n",
       " 'Alice',\n",
       " 'Alicia',\n",
       " 'All',\n",
       " 'Allan',\n",
       " 'Allen',\n",
       " 'Alliance',\n",
       " 'Allied',\n",
       " 'Allies',\n",
       " 'Allstates',\n",
       " 'Alma',\n",
       " 'Almagest',\n",
       " 'Almighty',\n",
       " 'Almost',\n",
       " 'Along',\n",
       " 'Alpert',\n",
       " 'Alpha',\n",
       " 'Already',\n",
       " 'Alsatian',\n",
       " 'Also',\n",
       " 'Alsop',\n",
       " 'Altenburg',\n",
       " 'Although',\n",
       " 'Altogether',\n",
       " 'Alumni',\n",
       " 'Alvin',\n",
       " 'Always',\n",
       " 'Amadee',\n",
       " 'Ambassador',\n",
       " 'Ambiguity',\n",
       " 'Amen',\n",
       " 'Amendment',\n",
       " 'America',\n",
       " \"America's\",\n",
       " 'American',\n",
       " 'Americans',\n",
       " 'Among',\n",
       " 'Amsterdam',\n",
       " 'Amy',\n",
       " 'An',\n",
       " 'Analysis',\n",
       " 'Analytical',\n",
       " 'Ancient',\n",
       " 'And',\n",
       " 'Anderson',\n",
       " 'Andover',\n",
       " 'Andre',\n",
       " 'Andrea',\n",
       " 'Andrei',\n",
       " 'Andrena',\n",
       " 'Andrew',\n",
       " 'Andrus',\n",
       " 'Andy',\n",
       " \"Andy's\",\n",
       " 'Angel',\n",
       " \"Angel's\",\n",
       " 'Angeles',\n",
       " 'Angelina',\n",
       " 'Angelo',\n",
       " 'Angels',\n",
       " 'Angie',\n",
       " 'Anglican',\n",
       " 'Anglo-Saxon',\n",
       " 'Angola',\n",
       " 'Angry',\n",
       " 'Animals',\n",
       " 'Ann',\n",
       " 'Annapolis',\n",
       " 'Anne',\n",
       " 'Anniston',\n",
       " 'Anniversary',\n",
       " 'Another',\n",
       " 'Answer',\n",
       " 'Anta',\n",
       " 'Anthea',\n",
       " 'Anthony',\n",
       " \"Antoine's\",\n",
       " 'Antonio',\n",
       " 'Any',\n",
       " 'Anybody',\n",
       " 'Anyhow',\n",
       " 'Anyone',\n",
       " 'Anything',\n",
       " 'Anyway',\n",
       " 'Apaches',\n",
       " 'Apart',\n",
       " 'Apollo',\n",
       " 'Apparently',\n",
       " 'Appeal',\n",
       " 'Appeals',\n",
       " 'Appendix',\n",
       " 'Application',\n",
       " 'Approved',\n",
       " 'Approximately',\n",
       " 'April',\n",
       " 'Arabic',\n",
       " 'Arbogast',\n",
       " 'Arbuckle',\n",
       " 'Arch',\n",
       " 'Archangel',\n",
       " 'Archbishop',\n",
       " 'Arctic',\n",
       " 'Are',\n",
       " 'Area',\n",
       " \"Aren't\",\n",
       " 'Argiento',\n",
       " 'Argon',\n",
       " 'Aricaras',\n",
       " 'Aristotle',\n",
       " \"Aristotle's\",\n",
       " 'Arithmetic',\n",
       " 'Ariz.',\n",
       " 'Arizona',\n",
       " 'Arkansas',\n",
       " 'Arlen',\n",
       " 'Arlene',\n",
       " 'Arlington',\n",
       " 'Armed',\n",
       " 'Arms',\n",
       " 'Armstrong',\n",
       " 'Army',\n",
       " \"Army's\",\n",
       " 'Arnold',\n",
       " 'Arnolphe',\n",
       " 'Around',\n",
       " 'Arp',\n",
       " 'Art',\n",
       " 'Arthur',\n",
       " 'Article',\n",
       " 'Artie',\n",
       " 'Artists',\n",
       " 'Arts',\n",
       " 'Arundel',\n",
       " 'As',\n",
       " 'Asia',\n",
       " 'Asian',\n",
       " 'Aside',\n",
       " 'Ask',\n",
       " 'Asked',\n",
       " 'Askington',\n",
       " 'Assemblies',\n",
       " 'Assembly',\n",
       " 'Assistant',\n",
       " 'Associate',\n",
       " 'Associated',\n",
       " 'Association',\n",
       " 'Assuming',\n",
       " 'At',\n",
       " 'Athabascan',\n",
       " 'Athenians',\n",
       " 'Athens',\n",
       " 'Athletic',\n",
       " 'Athletics',\n",
       " 'Atlanta',\n",
       " \"Atlanta's\",\n",
       " 'Atlantic',\n",
       " 'Atlas',\n",
       " 'Atomic',\n",
       " 'Attendance',\n",
       " 'Attorney',\n",
       " 'Attorneys',\n",
       " 'Atty.',\n",
       " 'Auditorium',\n",
       " 'Audubon',\n",
       " 'Aug.',\n",
       " 'August',\n",
       " 'Augusta',\n",
       " 'Augustine',\n",
       " 'Augustus',\n",
       " 'Aunt',\n",
       " 'Auntie',\n",
       " 'Aureomycin',\n",
       " 'Austin',\n",
       " 'Australia',\n",
       " 'Australian',\n",
       " 'Austria',\n",
       " 'Austrian',\n",
       " 'Author',\n",
       " 'Authorities',\n",
       " 'Authority',\n",
       " \"Authority's\",\n",
       " 'Autocoder',\n",
       " 'Autocracies',\n",
       " 'Automobile',\n",
       " 'Av.',\n",
       " 'Available',\n",
       " 'Ave.',\n",
       " 'Avenue',\n",
       " 'Aw',\n",
       " 'Award',\n",
       " 'Axis',\n",
       " 'B',\n",
       " \"B'dikkat\",\n",
       " 'B-52',\n",
       " 'B-70',\n",
       " 'B.',\n",
       " 'B.C.',\n",
       " 'BAM',\n",
       " 'BOD',\n",
       " 'BTU',\n",
       " 'Babe',\n",
       " 'Baby',\n",
       " 'Bach',\n",
       " 'Back',\n",
       " 'Bad',\n",
       " 'Baer',\n",
       " 'Bailey',\n",
       " 'Baird',\n",
       " 'Bake',\n",
       " 'Baker',\n",
       " 'Balafrej',\n",
       " 'Ball',\n",
       " 'Ballet',\n",
       " 'Baltic',\n",
       " 'Baltimore',\n",
       " 'Bancroft',\n",
       " 'Band',\n",
       " 'Bang-Jensen',\n",
       " \"Bang-Jensen's\",\n",
       " 'Bank',\n",
       " 'Banks',\n",
       " 'Baptist',\n",
       " 'Bar',\n",
       " 'Barbara',\n",
       " 'Barber',\n",
       " 'Barco',\n",
       " \"Barco's\",\n",
       " 'Bari',\n",
       " 'Barker',\n",
       " 'Barnard',\n",
       " 'Barnes',\n",
       " 'Barnett',\n",
       " 'Barney',\n",
       " 'Barnumville',\n",
       " 'Baroque',\n",
       " 'Barre',\n",
       " 'Barth',\n",
       " 'Barton',\n",
       " \"Barton's\",\n",
       " 'Bascom',\n",
       " 'Base',\n",
       " 'Baseball',\n",
       " 'Based',\n",
       " 'Basic',\n",
       " 'Bates',\n",
       " 'Batista',\n",
       " \"Batista's\",\n",
       " 'Baton',\n",
       " 'Battenkill',\n",
       " 'Battery',\n",
       " 'Battle',\n",
       " 'Baullari',\n",
       " 'Bay',\n",
       " 'Bayreuth',\n",
       " 'Be',\n",
       " 'Beach',\n",
       " 'Bear',\n",
       " 'Bearden',\n",
       " 'Beardens',\n",
       " 'Bears',\n",
       " 'Beatie',\n",
       " 'Beatrice',\n",
       " 'Beauclerk',\n",
       " 'Beaumont',\n",
       " 'Beautiful',\n",
       " 'Beauty',\n",
       " 'Beaverton',\n",
       " 'Because',\n",
       " 'Beckett',\n",
       " \"Beckett's\",\n",
       " 'Beckstrom',\n",
       " 'Beckworth',\n",
       " 'Bede',\n",
       " 'Beebe',\n",
       " 'Beech',\n",
       " 'Beer',\n",
       " 'Beethoven',\n",
       " 'Before',\n",
       " 'Beginning',\n",
       " 'Behind',\n",
       " 'Being',\n",
       " 'Belgian',\n",
       " 'Belgians',\n",
       " 'Bell',\n",
       " 'Bellini',\n",
       " 'Bellows',\n",
       " 'Below',\n",
       " 'Ben',\n",
       " 'Bench',\n",
       " 'Bend',\n",
       " 'Beneath',\n",
       " 'Bengal',\n",
       " 'Benington',\n",
       " 'Benjamin',\n",
       " 'Bennington',\n",
       " 'Benson',\n",
       " 'Beowulf',\n",
       " 'Berger',\n",
       " \"Berger's\",\n",
       " 'Berle',\n",
       " 'Berlin',\n",
       " 'Berman',\n",
       " 'Bermuda',\n",
       " 'Bern',\n",
       " 'Bernard',\n",
       " 'Berry',\n",
       " 'Bertha',\n",
       " 'Berto',\n",
       " 'Besides',\n",
       " 'Best',\n",
       " 'Beth',\n",
       " 'Better',\n",
       " 'Between',\n",
       " 'Beverly',\n",
       " 'Bey',\n",
       " 'Beyond',\n",
       " 'Bible',\n",
       " 'Biblical',\n",
       " 'Bienville',\n",
       " 'Big',\n",
       " 'Bill',\n",
       " 'Billy',\n",
       " 'Biological',\n",
       " 'Bird',\n",
       " 'Birds',\n",
       " 'Birmingham',\n",
       " 'Bishop',\n",
       " 'Bisque',\n",
       " 'Black',\n",
       " \"Black's\",\n",
       " 'Blackman',\n",
       " 'Blackwell',\n",
       " 'Blake',\n",
       " 'Blakey',\n",
       " 'Blanche',\n",
       " 'Blatz',\n",
       " 'Blenheim',\n",
       " 'Blessed',\n",
       " 'Blockade',\n",
       " 'Blood',\n",
       " 'Blue',\n",
       " 'Blues',\n",
       " 'Blvd.',\n",
       " 'Board',\n",
       " 'Boats',\n",
       " 'Bob',\n",
       " 'Bobbie',\n",
       " 'Bobby',\n",
       " 'Body',\n",
       " 'Bomb',\n",
       " 'Bombus',\n",
       " 'Bond',\n",
       " 'Bondi',\n",
       " 'Bong',\n",
       " 'Boniface',\n",
       " 'Bonn',\n",
       " 'Bonner',\n",
       " 'Bonnor',\n",
       " 'Book',\n",
       " 'Books',\n",
       " 'Borden',\n",
       " 'Border',\n",
       " 'Boris',\n",
       " \"Boris'\",\n",
       " 'Born',\n",
       " 'Bosis',\n",
       " 'Bosphorus',\n",
       " 'Boston',\n",
       " 'Both',\n",
       " 'Bottom',\n",
       " 'Boulevard',\n",
       " 'Boun',\n",
       " 'Boundary',\n",
       " 'Bourbon',\n",
       " 'Bourbons',\n",
       " 'Bourcier',\n",
       " 'Bowman',\n",
       " 'Box',\n",
       " 'Boxell',\n",
       " 'Boy',\n",
       " 'Boyd',\n",
       " 'Boys',\n",
       " 'Brace',\n",
       " 'Bradford',\n",
       " 'Bradley',\n",
       " 'Brahms',\n",
       " 'Brain',\n",
       " 'Branch',\n",
       " 'Brandon',\n",
       " 'Brandt',\n",
       " 'Brandywine',\n",
       " 'Brannon',\n",
       " 'Braque',\n",
       " 'Brassnose',\n",
       " 'Brave',\n",
       " 'Braves',\n",
       " 'Brazil',\n",
       " 'Brazilian',\n",
       " 'Breasted',\n",
       " 'Breath',\n",
       " 'Breeding',\n",
       " 'Brenner',\n",
       " \"Brenner's\",\n",
       " 'Brett',\n",
       " 'Brevard',\n",
       " 'Bridge',\n",
       " 'Bridges',\n",
       " 'Bridget',\n",
       " 'Brief',\n",
       " 'Bright',\n",
       " 'Brisbane',\n",
       " 'Bristol',\n",
       " 'Britain',\n",
       " \"Britain's\",\n",
       " 'British',\n",
       " 'Broadcasting',\n",
       " 'Broadway',\n",
       " 'Brodie',\n",
       " 'Bronx',\n",
       " 'Brooklyn',\n",
       " 'Brooks',\n",
       " 'Brothers',\n",
       " 'Broun',\n",
       " 'Brown',\n",
       " \"Brown's\",\n",
       " 'Browning',\n",
       " 'Bruce',\n",
       " 'Bruckner',\n",
       " \"Bruckner's\",\n",
       " 'Brumidi',\n",
       " \"Brumidi's\",\n",
       " 'Bryan',\n",
       " 'Buchheister',\n",
       " 'Buck',\n",
       " 'Buckley',\n",
       " 'Bud',\n",
       " 'Budapest',\n",
       " 'Budd',\n",
       " 'Buddha',\n",
       " 'Buddhism',\n",
       " 'Buffalo',\n",
       " 'Bugs',\n",
       " ...]"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import string\n",
    "from collections import defaultdict\n",
    "\n",
    "training_data = []\n",
    "def build_vocab_new(): #fujnction for creating the vocabulary using the training set\n",
    "    vocab = []\n",
    "    freqs = defaultdict(int)\n",
    "    for i in range(len(data)):\n",
    "        sentence = data[i]\n",
    "        for token, tag in sentence:\n",
    "            # Append (token, tag) tuple to training_data\n",
    "            training_data.append((token, tag))\n",
    "            freqs[token]+=1\n",
    "            \n",
    "    vocab = [k for k, v in freqs.items() if (v>2 and k!='\\n')] #check the condition again\n",
    "    unk_toks = [\"--unk--\", \"--unk_adj--\", \"--unk_adv--\", \"--unk_digit--\", \"--unk_noun--\", \"--unk_punct--\", \"--unk_upper--\", \"--unk_verb--\"]\n",
    "    vocab.extend(unk_toks)\n",
    "    vocab.append(\"--n--\") #represents end of sentence\n",
    "    vocab.append(\" \") #represents the space character\n",
    "    vocab = sorted(set(vocab))\n",
    "#     print(vocab[0:50])\n",
    "#     print(vocab[-50:])\n",
    "    return vocab\n",
    "\n",
    "build_vocab_new()\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "2ce47ae7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vocabulary dictionary, key is the word, value is a unique integer\n",
      " :0\n",
      "!:1\n",
      "$.03:2\n",
      "$1:3\n",
      "$1,000:4\n",
      "$1,500:5\n",
      "$10:6\n",
      "$10,000:7\n",
      "$100:8\n",
      "$135:9\n",
      "$14:10\n",
      "$15:11\n",
      "$15,000:12\n",
      "$150:13\n",
      "$2:14\n",
      "$2,000:15\n",
      "$20:16\n",
      "$20,000:17\n",
      "$200:18\n",
      "$25:19\n",
      "$25,000:20\n"
     ]
    }
   ],
   "source": [
    "# vocab: dictionary that has the index of the corresponding words-each word in the vocab is linked to an integer\n",
    "vocab = {} \n",
    "\n",
    "# Get the index of the corresponding words. \n",
    "for i, word in enumerate(sorted(build_vocab_new())): \n",
    "    vocab[word] = i       \n",
    "    \n",
    "print(\"Vocabulary dictionary, key is the word, value is a unique integer\")\n",
    "cnt = 0\n",
    "for k,v in vocab.items():\n",
    "    print(f\"{k}:{v}\")\n",
    "    cnt += 1\n",
    "    if cnt > 20:\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "984e0917",
   "metadata": {},
   "outputs": [],
   "source": [
    "import string\n",
    "# punctuation characters\n",
    "punct = set(string.punctuation)\n",
    "\n",
    "# morphology rules used to assign unknown word tokens\n",
    "noun_suffix = [\"action\", \"age\", \"ance\", \"cy\", \"dom\", \"ee\", \"ence\", \"er\", \"hood\", \"ion\", \"ism\", \"ist\", \"ity\", \"ling\", \"ment\", \"ness\", \"or\", \"ry\", \"scape\", \"ship\", \"ty\"]\n",
    "verb_suffix = [\"ate\", \"ify\", \"ise\", \"ize\"]\n",
    "adj_suffix = [\"able\", \"ese\", \"ful\", \"i\", \"ian\", \"ible\", \"ic\", \"ish\", \"ive\", \"less\", \"ly\", \"ous\"]\n",
    "adv_suffix = [\"ward\", \"wards\", \"wise\"]\n",
    "\n",
    "def assign_unk(tok):\n",
    "    # Digits\n",
    "    if any(char.isdigit() for char in tok):\n",
    "        return \"--unk_digit--\"\n",
    "\n",
    "    # Punctuation\n",
    "    elif any(char in punct for char in tok):\n",
    "        return \"--unk_punct--\"\n",
    "\n",
    "    # Upper-case\n",
    "    elif any(char.isupper() for char in tok):\n",
    "        return \"--unk_upper--\"\n",
    "\n",
    "    # Nouns\n",
    "    elif any(tok.endswith(suffix) for suffix in noun_suffix):\n",
    "        return \"--unk_noun--\"\n",
    "\n",
    "    # Verbs\n",
    "    elif any(tok.endswith(suffix) for suffix in verb_suffix):\n",
    "        return \"--unk_verb--\"\n",
    "\n",
    "    # Adjectives\n",
    "    elif any(tok.endswith(suffix) for suffix in adj_suffix):\n",
    "        return \"--unk_adj--\"\n",
    "\n",
    "    # Adverbs\n",
    "    elif any(tok.endswith(suffix) for suffix in adv_suffix):\n",
    "        return \"--unk_adv--\"\n",
    "\n",
    "    return \"--unk--\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "2179c6a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "#function for creating dictionaries for storing the count of tags, words to be used for finding probabilities\n",
    "def create_dictionaries(training_data, vocab):\n",
    "    emission_counts = defaultdict(int)\n",
    "    transition_counts = defaultdict(int)\n",
    "    tag_counts = defaultdict(int)\n",
    "\n",
    "    prev_tag = '--s--'\n",
    "    tag_counts[prev_tag] = 1\n",
    "    for token, tag in training_data:\n",
    "        if(token not in vocab):\n",
    "            token = assign_unk(token)\n",
    "        transition_counts[(prev_tag, tag)] += 1\n",
    "        emission_counts[(tag, tok)] += 1\n",
    "        tag_counts[tag] += 1\n",
    "        prev_tag = tag\n",
    "            \n",
    "    return emission_counts, transition_counts, tag_counts\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "900b6b43",
   "metadata": {},
   "outputs": [],
   "source": [
    "# for creating transition and emission matrix \n",
    "\n",
    "def create_transition_matrix(alpha, tag_counts, transition_counts):\n",
    "    all_tags = sorted(tag_counts.keys())\n",
    "    num_tags = len(all_tags)\n",
    "\n",
    "    # initialize the transition matrix 'A'\n",
    "    A = np.zeros((num_tags, num_tags))\n",
    "\n",
    "    # get the unique transition tuples (prev POS, cur POS)\n",
    "    trans_keys = set(transition_counts.keys())\n",
    "\n",
    "    for i in range(num_tags):\n",
    "        for j in range(num_tags):\n",
    "            # initialize the count of (prev POS, cur POS)\n",
    "            count = 0\n",
    "\n",
    "            key = (all_tags[i], all_tags[j])\n",
    "            if key in transition_counts:\n",
    "                count = transition_counts[key]\n",
    "            count_prev_tag = tag_counts[all_tags[i]]\n",
    "\n",
    "            A[i, j] = (count + alpha) / (count_prev_tag + alpha * num_tags)\n",
    "\n",
    "    return A\n",
    "\n",
    "def create_emission_matrix(alpha, tag_counts, emission_counts, vocab):\n",
    "    num_tags = len(tag_counts)\n",
    "    all_tags = sorted(tag_counts.keys())\n",
    "    num_words = len(vocab)\n",
    "\n",
    "    B = np.zeros((num_tags, num_words))\n",
    "    emis_keys = set(list(emission_counts.keys()))\n",
    "    for i in range(num_tags):\n",
    "        for j in range(num_words):\n",
    "            count = 0\n",
    "\n",
    "            key =  (all_tags[i], vocab2idx[j])\n",
    "            if key in emission_counts:\n",
    "                count = emission_counts[key]\n",
    "            count_tag = tag_counts[all_tags[i]]\n",
    "\n",
    "            B[i, j] = (count + alpha) / (count_tag + alpha * num_words)\n",
    "    return B\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "d741e1b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# for verterbi algorithm\n",
    "\n",
    "def initialize(A, B, tag_counts, vocab, states, prep_tokens):\n",
    "    num_tags = len(tag_counts)\n",
    "    best_probs = np.zeros((num_tags, len(prep_tokens)))\n",
    "    best_paths = np.zeros((num_tags, len(prep_tokens)), dtype=int)\n",
    "    s_idx = states.index('--s--')\n",
    "\n",
    "    for i in range(num_tags):\n",
    "        if A[s_idx, i] == 0:\n",
    "            best_probs[i, 0] = float('-inf')\n",
    "        else:\n",
    "            best_probs[i,0] = np.log(A[s_idx, i]) + \n",
    "            if prep_tokens[0] is in vocab:\n",
    "                best_probs[i,0]+=np.log(B[i, vocab[prep_tokens[0]]])\n",
    "\n",
    "    return best_probs, best_paths\n",
    "\n",
    "def viterbi_forward(A, B, prep_tokens, best_probs, best_paths, vocab):\n",
    "    num_tags = best_probs.shape[0]\n",
    "    for i in range(1, len(prep_tokens)):\n",
    "        for j in range(num_tags):\n",
    "            best_prob_i = float('-inf')\n",
    "            best_path_i = None\n",
    "\n",
    "            for k in range(num_tags):\n",
    "                prob = best_probs[k,i-1]+np.log(A[k,j])\n",
    "                if prep_tokens[i] is in vocab:\n",
    "                    prob+=np.log(B[j,vocab[prep_tokens[i]]])\n",
    "                if prob > best_prob_i:\n",
    "                    best_prob_i = prob\n",
    "                    best_path_i = k\n",
    "            best_probs[j, i] = best_prob_i\n",
    "            best_paths[j, i] = best_path_i\n",
    "    return best_probs, best_paths\n",
    "\n",
    "\n",
    "def viterbi_backward(best_probs, best_paths, states):\n",
    "    m = best_paths.shape[1]\n",
    "    z = [None] * m\n",
    "    num_tags = best_probs.shape[0]\n",
    "\n",
    "    best_prob_for_last_word = float('-inf')\n",
    "    pred = [None] * m\n",
    "\n",
    "    for k in range(num_tags):\n",
    "        if best_probs[k, m - 1] > best_prob_for_last_word:\n",
    "            best_prob_for_last_word = best_probs[k, m - 1]\n",
    "            z[m - 1] = k\n",
    "    pred[m - 1] = states[z[m - 1]]\n",
    "\n",
    "    for i in range(m-1, -1, -1):\n",
    "        pos_tag_for_word_i = z[i]\n",
    "        z[i - 1] = best_paths[pos_tag_for_word_i,i]\n",
    "        pred[i - 1] = states[z[i - 1]]\n",
    "    return pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "27fd48ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "def hmm_tagger_util(sent_id, untagged_sentence):\n",
    "    emission_counts, transition_counts, tag_counts = create_dictionaries(training_data, vocab)\n",
    "    states = sorted(tag_counts.keys())\n",
    "#     print(f\"Number of POS tags (number of 'states'): {len(states)}\")\n",
    "#     print(\"View these POS tags (states)\")\n",
    "#     print(states)\n",
    "    alpha = 0.001 #need to tune it\n",
    "    A = create_transition_matrix(alpha, tag_counts, transition_counts)\n",
    "    B = create_emission_matrix(alpha, tag_counts, emission_counts, list(vocab))\n",
    "    best_probs, best_paths = initialize(states, tag_counts, A, B, prep, vocab)\n",
    "    best_probs, best_paths = viterbi_forward(A, B, prep, best_probs, best_paths, vocab)\n",
    "    pred = viterbi_backward(best_probs, best_paths, untagged_sentence, states) #prep is the one unatgged sentence from test data set\n",
    "    m=len(pred)\n",
    "    #pred is the list of tags for untagged_sentence\n",
    "    tagged_sentence_pred = []\n",
    "    for i in range(len(pred)):\n",
    "        taggeg_sentence_new.append((untagged_sentence[i],pred[i]))\n",
    "        \n",
    "#     print('The prediction for pred[-7:m-1] is: \\n', prep[-7:m-1], \"\\n\", pred[-7:m-1], \"\\n\")\n",
    "#     print('The prediction for pred[0:8] is: \\n', pred[0:7], \"\\n\", untagged_sentence[0:7])\n",
    "    store_submission(sent_id, tagged_sentence_new)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61e2deab",
   "metadata": {},
   "outputs": [],
   "source": [
    "for id_, sentence in test_data.items():\n",
    "    hmm_tagger_util(id_, sentence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "0f6bdf6f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Memm tagger"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "dba3d8c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Code for feature extraction\n",
    "\n",
    "import sys\n",
    "from collections import Counter\n",
    "import time\n",
    "\n",
    "features_file = \"feature_file\"\n",
    "\n",
    "UNK_TOKEN = 'UNK'\n",
    "START_TOKEN = '<START>'\n",
    "START_TAG = '<S>'\n",
    "END_TOKEN = '<END>'\n",
    "\n",
    "\n",
    "def extract(sent, i, last_tags, rare_or_unknown):\n",
    "\n",
    "    _sent = [START_TOKEN, START_TOKEN] + sent + [END_TOKEN, END_TOKEN]\n",
    "    _i = i + 2\n",
    "\n",
    "    word = _sent[_i]\n",
    "\n",
    "    features = dict(prev_prev_t=last_tags[0],\n",
    "                    prev_t=last_tags[1],\n",
    "                    word=word if not rare_or_unknown else UNK_TOKEN,\n",
    "                    pref1=word[:1],\n",
    "                    pref2='' if len(word) < 2 else word[:2],\n",
    "                    pref3='' if len(word) < 3 else word[:3],\n",
    "                    pref4='' if len(word) < 4 else word[:4],\n",
    "                    pref5='' if len(word) < 5 else word[:5],\n",
    "                    pref6='' if len(word) < 6 else word[:6],\n",
    "                    suff6='' if len(word) < 6 else word[-6:],\n",
    "                    suff5='' if len(word) < 5 else word[-5:],\n",
    "                    suff4='' if len(word) < 4 else word[-4:],\n",
    "                    suff3='' if len(word) < 3 else word[-3:],\n",
    "                    suff2='' if len(word) < 2 else word[-2:],\n",
    "                    suff1=word[-1:],\n",
    "                    prev_prev_w=_sent[_i - 2],\n",
    "                    prev_w=_sent[_i - 1],\n",
    "                    next_w=_sent[_i + 1],\n",
    "                    next_next_w=_sent[_i + 2])\n",
    "\n",
    "    return features\n",
    "\n",
    "\n",
    "def identify_rare_words(words, num_occurrences=1):\n",
    "\n",
    "    rare = set()\n",
    "\n",
    "    # Count the number of occurrences of each word in the training set.\n",
    "    counter = Counter(words)\n",
    "\n",
    "    # Collect the words in the training set that appear only once and consider them as rare wards.\n",
    "    for word, amount in counter.items():\n",
    "        if amount <= num_occurrences:\n",
    "            rare.add(word)\n",
    "\n",
    "    return rare"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "92963bd2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('The', 'AT'), ('jury', 'NN'), ('further', 'RB'), ('said', 'VB'), ('in', 'IN'), ('term-end', 'NN'), ('presentments', 'NN'), ('that', 'CS'), ('the', 'AT'), ('City', 'NN'), ('Executive', 'JJ'), ('Committee', 'NN'), (',', ','), ('which', 'WD'), ('had', 'HV'), ('over-all', 'JJ'), ('charge', 'NN'), ('of', 'IN'), ('the', 'AT'), ('election', 'NN'), (',', ','), ('``', '``'), ('deserves', 'VB'), ('the', 'AT'), ('praise', 'NN'), ('and', 'CC'), ('thanks', 'NN'), ('of', 'IN'), ('the', 'AT'), ('City', 'NN'), ('of', 'IN'), ('Atlanta', 'NP'), (\"''\", \"''\"), ('for', 'IN'), ('the', 'AT'), ('manner', 'NN'), ('in', 'IN'), ('which', 'WD'), ('the', 'AT'), ('election', 'NN'), ('was', 'BE'), ('conducted', 'VB'), ('.', '.')]\n"
     ]
    }
   ],
   "source": [
    "print(data[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "1ba8c4dc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Feature Extraction finished in 21.06 seconds\n"
     ]
    }
   ],
   "source": [
    "start_time = time.time()\n",
    "\n",
    "corpus = data\n",
    "\n",
    "# Parse the input sentences and identify the rare words.\n",
    "train_words = []\n",
    "for i in range(len(data)):\n",
    "    sentence = data[i]\n",
    "    for token, tag in sentence:\n",
    "        train_words.append(token)\n",
    "            \n",
    "rare_words = identify_rare_words(train_words)\n",
    "\n",
    "with open(features_file, \"w\") as features_file:\n",
    "    for i in range(len(data)):\n",
    "        sentence = data[i]\n",
    "\n",
    "        # Parse the current sentence to words and tags.\n",
    "        tuples = sentence\n",
    "        words, tags = [pair[0] for pair in tuples], [START_TAG, START_TAG] + [pair[1] for pair in tuples]\n",
    "\n",
    "        # For each word in the sentence.\n",
    "        for idx in range(len(tuples)):\n",
    "\n",
    "            # Update the last two predicted tags.\n",
    "            last_two_tags = tags[idx], tags[idx + 1]\n",
    "\n",
    "            # Extract the word's features.\n",
    "            features = extract(words, idx, last_two_tags, words[idx] in rare_words)\n",
    "\n",
    "            # Write the word's features to the features file.\n",
    "            features_file.write(\n",
    "                tags[idx + 2] + ' ' + ' '.join([key + \"=\" + str(val) for key, val in features.items()]) + \"\\n\")\n",
    "\n",
    "passed_time = time.time() - start_time\n",
    "print(\"Feature Extraction finished in %.2f seconds\" % passed_time)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "35ee8827",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Dell\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\scipy\\__init__.py:146: UserWarning: A NumPy version >=1.17.3 and <1.25.0 is required for this version of SciPy (detected version 1.26.1\n",
      "  warnings.warn(f\"A NumPy version >={np_minversion} and <{np_maxversion}\"\n"
     ]
    }
   ],
   "source": [
    "import pickle\n",
    "import sys\n",
    "import time\n",
    "from sklearn.feature_extraction import DictVectorizer\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "features_file = \"feature_file\"\n",
    "model_file = \"model_file\"\n",
    "\n",
    "words_set = set()\n",
    "\n",
    "\n",
    "def load_features_file():\n",
    "\n",
    "    words_features, tags = [], []\n",
    "\n",
    "    # Fetch the training set's words features.\n",
    "    with open(features_file, \"r\", encoding=\"utf-8\") as featuresFile:\n",
    "        features = featuresFile.readlines()\n",
    "\n",
    "    # For each line in the features file.\n",
    "    for line in features:\n",
    "\n",
    "        # Parse the line into the word's features.\n",
    "        features = line.strip().split(' ')\n",
    "        tags.append(features.pop(0))\n",
    "\n",
    "        # Save the word's features in a dictionary.\n",
    "        featuresDict = dict([tuple(pair.split('=', 1)) for pair in features])\n",
    "        words_set.add(featuresDict['word'])\n",
    "        words_features.append(featuresDict)\n",
    "\n",
    "    return words_features, tags\n",
    "\n",
    "\n",
    "def convert_features_to_vectors(features):\n",
    "\n",
    "    v = DictVectorizer()\n",
    "\n",
    "    # Convert all words features into feature vectors.\n",
    "    feature_vectors = v.fit_transform(features)\n",
    "\n",
    "    # Save the DictVectorizer object to a file for using later in the prediction.\n",
    "    save_to_file(v)\n",
    "\n",
    "    return feature_vectors\n",
    "\n",
    "\n",
    "def save_to_file(feature_map):\n",
    "\n",
    "    # Save the DictVectorizer object and the set of  words appeared on the training set to a file.\n",
    "    with open('feature_map_file', 'wb') as file:\n",
    "        pickle.dump(feature_map, file)\n",
    "        pickle.dump(words_set, file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5c5b8e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "start_time = time.time()\n",
    "\n",
    "# Load all features from the features file and convert them into feature vectors.\n",
    "features_per_line, tags = load_features_file()\n",
    "feature_vectors = convert_features_to_vectors(features_per_line)\n",
    "\n",
    "# Logistic Regression classifier.\n",
    "log_reg = LogisticRegression(solver='lbfgs', multi_class='multinomial', penalty='l2', tol=1e-4, random_state=0,\n",
    "                             max_iter=1200) #need to tune them\n",
    "\n",
    "# Train the classifier.\n",
    "log_reg.fit(feature_vectors, tags)\n",
    "\n",
    "# Save the trained model to model_file.\n",
    "pickle.dump(log_reg, open(model_file, 'wb'))\n",
    "\n",
    "passed_time = time.time() - start_time\n",
    "print(\"Training finished in %.2f minutes\" % (passed_time / 60))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "896e97ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "import time\n",
    "\n",
    "def memm_tag_sentence(untagged_sentence, model_file_name=\"model_file\", feature_map_file=\"feature_map_file\"):\n",
    "    UNK_TOKEN = 'UNK'\n",
    "    START_TOKEN = '<START>'\n",
    "    START_TAG = '<S>'\n",
    "    END_TOKEN = '<END>'\n",
    "\n",
    "    start_time = time.time()\n",
    "\n",
    "    # Load the DictVectorizer object and the training set words.\n",
    "    with open(feature_map_file, 'rb') as file:\n",
    "        v = pickle.load(file)\n",
    "        train_words = pickle.load(file)\n",
    "\n",
    "    # Load the trained Logistic Regression model.\n",
    "    log_reg = pickle.load(open(model_file_name, 'rb'))\n",
    "\n",
    "    # Initialize last two tags for the sentence.\n",
    "    last_two_tags = [(START_TAG, START_TAG)]\n",
    "\n",
    "    # Initialize tagged sentence\n",
    "    tagged_sentence = []\n",
    "\n",
    "    # Iterate over each word in the untagged sentence\n",
    "    for i, word in enumerate(untagged_sentence):\n",
    "        # Extract features for the current word\n",
    "        features = extract(untagged_sentence, i, last_two_tags[0], not word in train_words)\n",
    "\n",
    "        # Convert features into a feature vector\n",
    "        feature_vector = v.transform([features])\n",
    "\n",
    "        # Predict the tag for the current word\n",
    "        pred_tag = log_reg.predict(feature_vector)[0]\n",
    "\n",
    "        # Append (word, tag) tuple to the tagged sentence\n",
    "        tagged_sentence.append((word, pred_tag))\n",
    "\n",
    "        # Update last two tags for next iteration\n",
    "        last_two_tags = (last_two_tags[1], pred_tag)\n",
    "\n",
    "    passed_time = time.time() - start_time\n",
    "    print(\"Prediction finished in %.2f seconds\" % passed_time)\n",
    "\n",
    "    return tagged_sentence\n",
    "\n",
    "# # Usage example:\n",
    "# test_data = [\"This\", \"is\", \"an\", \"example\", \"sentence\"]\n",
    "# tagged_result = tag_sentence(test_data)\n",
    "# print(tagged_result)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76932d96",
   "metadata": {},
   "outputs": [],
   "source": [
    "def memm_tagger_util(sent_id, untagged_sentence):\n",
    "    memm_tag_sentence(untagged_sentence, \"model_file\", \"feature_map_file\")\n",
    "    store_submission(sent_id, tagged_sentence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c08639d",
   "metadata": {},
   "outputs": [],
   "source": [
    "for id_, sentence in test_data.items():\n",
    "    memm_tagger_util(id_, sentence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d00b8935",
   "metadata": {},
   "outputs": [],
   "source": [
    "path_to_directory = '/kaggle/working/'\n",
    "pd.DataFrame(submission).to_csv(path_to_directory +' sample_submission.csv', index = False)"
   ]
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "none",
   "dataSources": [
    {
     "databundleVersionId": 7557019,
     "sourceId": 67975,
     "sourceType": "competition"
    }
   ],
   "dockerImageVersionId": 30635,
   "isGpuEnabled": false,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  },
  "papermill": {
   "default_parameters": {},
   "duration": 22.613679,
   "end_time": "2024-01-24T06:27:40.841939",
   "environment_variables": {},
   "exception": null,
   "input_path": "__notebook__.ipynb",
   "output_path": "__notebook__.ipynb",
   "parameters": {},
   "start_time": "2024-01-24T06:27:18.228260",
   "version": "2.4.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
